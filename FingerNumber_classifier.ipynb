{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FingerNumber_classifier.ipynb",
      "provenance": [],
      "mount_file_id": "1CiGp008K7eMHjk3Q-LtShkxNTxxbKZoX",
      "authorship_tag": "ABX9TyPvJeCDr1qN10Mi1HC2MHyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongjiGo/FingerNumber_classifier/blob/master/FingerNumber_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8xXWfV86dgA",
        "outputId": "22353166-da0b-4063-df2e-fe00ad205096"
      },
      "source": [
        "print(\"FingerNumber_Classifier\")\n",
        "print(\"[0] 모델 새로 학습하기\")\n",
        "print(\"[1] 학습된 모델로 새로운 사진 분류해보기\")\n",
        "print(\"[any] 종료\")\n",
        "\n",
        "menu = input(\"숫자를 입력해주세요: \")\n",
        "\n",
        "if menu == '0':\n",
        "    print(\"모델을 새로 학습합니다.\")\n",
        "    print(\"15 에포크 기준 정확도는 약 79%입니다.\")\n",
        "    epochNum = int(input(\"원하는 에포크 수를 입력해주세요: \"))\n",
        "    print(f\"{epochNum}으로 에포크를 설정하였습니다.\")\n",
        "    print(\"모델 학습을 시작합니다.\")\n",
        "\n",
        "elif menu == '1':\n",
        "    print(\"분류를 진행합니다.\")\n",
        "    file_path = input(\"파일의 [절대경로]를 입력해주세요: \")\n",
        "\n",
        "else:\n",
        "    print('프로그램을 종료합니다.')\n",
        "\n",
        "if menu == '0':\n",
        "    # 데이터 셋 구성하기, 경로를 파악한 후\n",
        "    # 클래스 이름(name), 클래스(class), 그리고 학습을 위한 클래스를 숫자로 나타낸 타겟(target)을 csv파일에 저장\n",
        "    import os\n",
        "    from glob import glob # 인자로 받은 패턴과 이름이 일치하는 모든 파일과 디렉터리의 리스트 반환\n",
        "    import pandas as pd\n",
        "\n",
        "    file_path = './drive/MyDrive/FingerNumber_classifier_project/dataSet/*/*.png' # 데이터의 경로 저장\n",
        "    file_list = glob(file_path)\n",
        "\n",
        "    data_dict = {'image_name':[], 'class':[], 'target':[], 'file_path':[]}\n",
        "    # 학습에 사용하기 위한 넘버링(?)\n",
        "    target_dict = {'yi_1': 0, 'er_2': 1, 'san_3': 2, 'si_4':3, 'wu_5':4, 'liu_6':5, 'qi_7':6, 'ba_8':7, 'jiu_9':8, 'shi_10': 9}\n",
        "\n",
        "    for path in file_list:\n",
        "        data_dict['file_path'].append(path) # file_path 항목에 파일 경로 저장\n",
        "\n",
        "        path_list = path.split(os.path.sep) # os별 파일 경로 구분 문자로 split\n",
        "\n",
        "        data_dict['image_name'].append(path_list[-1]) # 이미지 이름 저장\n",
        "        data_dict['class'].append(path_list[-2]) # 어떤 클래스인지 저장\n",
        "        data_dict['target'].append(target_dict[path_list[-2]]) # 그 클래스의 번호 저장\n",
        "\n",
        "    train_df = pd.DataFrame(data_dict) # 데이터 프레임 화\n",
        "    train_df.to_csv(\"./drive/MyDrive/FingerNumber_classifier_project/train.csv\", mode='w') # csv파일로 생성\n",
        "    print('csv파일 생성 완료!')\n",
        "\n",
        "    from sklearn.model_selection import train_test_split # 스플릿 모듈\n",
        "    def get_df():\n",
        "        # csv 파일 읽어서 DataFrame으로 저장\n",
        "        df = pd.read_csv(\"./drive/MyDrive/FingerNumber_classifier_project/train.csv\") # csv로 불러와서 데이터 저장\n",
        "        print('csv 파일 DataFrame으로 저장 완료!')\n",
        "\n",
        "        # 데이터셋을 train, val, test로 나누기\n",
        "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=2359)\n",
        "        df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=2359)\n",
        "        print('훈련셋, 검증셋, 테스트셋 분할 완료!')\n",
        "        return df_train, df_val, df_test\n",
        "\n",
        "    # 데이터셋 읽어오기\n",
        "    df_train, df_val, df_test = get_df()\n",
        "    print(f'훈련셋 개수:{len(df_train)}, 검증셋 개수:{len(df_val)}, 테스트셋 개수: {len(df_test)}') # 192, 48, 60\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import Dataset\n",
        "    from PIL import Image\n",
        "\n",
        "    # 학습시, 데이터셋을 사용할 수 있도록 만들기\n",
        "    class Classification_Dataset(Dataset):\n",
        "        def __init__(self, csv, mode, transform=None):\n",
        "            self.csv = csv.reset_index(drop=True) # random으로 섞인 데이터의 인덱스를 reset 시켜서 다시 부여한다.\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.csv.shape[0] # csv 파일의 행 개수 == 데이터 개수\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            row = self.csv.iloc[index] # 주어진 index에 대한 데이터 뽑아오기\n",
        "            image = Image.open(row.file_path).convert('RGB') # 파일 경로로 부터 이미지를 읽고 rgb로 변환하기\n",
        "            target = torch.tensor(self.csv.iloc[index].target).long()\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image) # 이미지에 transform 적용하기\n",
        "\n",
        "            return image, target # 이미지와 target return하기기\n",
        "\n",
        "\n",
        "    # normalize를 위해 rgb 채널의 mean, std 값 구하기\n",
        "\n",
        "    import numpy as np\n",
        "    from torchvision import transforms\n",
        "    dataset_train = Classification_Dataset(df_train, 'train', transform=transforms.ToTensor())\n",
        "\n",
        "    # 데이터(shape:torch.Size([3, 381, 343])) rgb에 대한 mean, std 구하기\n",
        "    rgb_mean = [np.mean(x.numpy(), axis=(1, 2)) for x, _ in dataset_train]\n",
        "    rgb_std = [np.std(x.numpy(), axis=(1, 2)) for x, _ in dataset_train]\n",
        "\n",
        "    # 각 데이터 채널별로 mean, std 나타내기\n",
        "    c_mean = []\n",
        "    c_std = []\n",
        "    for i in range(3):\n",
        "        c_mean.append(np.mean([m[i] for m in rgb_mean]))\n",
        "        c_std.append(np.std([s[i] for s in rgb_std]))\n",
        "\n",
        "    print('rgb의 mean, std값 계산 완료!')\n",
        "    # 사용자 모델 트랜스폼\n",
        "    def get_transforms(image_size):\n",
        "        transforms_train = transforms.Compose([\n",
        "            transforms.RandomRotation(30), # 이미지의 다양화를 위해 랜덤으로 +- 30도 가량 회전\n",
        "            transforms.RandomResizedCrop(image_size), # 이미지 사이즈 축소\n",
        "            transforms.RandomHorizontalFlip(), # 이미지를 랜덤으로 수평하게 뒤집음.\n",
        "            transforms.ToTensor(), # 데이터 타입을 텐서로 변경\n",
        "            transforms.Normalize(c_mean, c_std) ]) # 정규화\n",
        "\n",
        "        transforms_val = transforms.Compose([transforms.Resize(image_size + 30), # 이미지 사이즈 축소\n",
        "                                             transforms.CenterCrop(image_size), # 이미지의 가운데 부분을 인자값으로 자름\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize(c_mean, c_std)])\n",
        "        transform_test = transforms.Compose([transforms.Resize(image_size)])\n",
        "        return transforms_train, transforms_val\n",
        "\n",
        "    # 모델 트랜스폼 가져오기\n",
        "    transforms_train, transforms_val = get_transforms(224)\n",
        "    print(\"모델 트랜스폼 불러오기 완료!\")\n",
        "\n",
        "    # dataset class 객체 만들기\n",
        "    dataset_train = Classification_Dataset(df_train, 'train', transform=transforms_train)\n",
        "    dataset_val = Classification_Dataset(df_val, 'valid', transform=transforms_val)\n",
        "    dataset_test = Classification_Dataset(df_test, 'test') # 트랜스폼 적용할 필요 없음\n",
        "    print('dataset class 객체 생성 완료!')\n",
        "\n",
        "    # DataLoader는 Classification_Dataset으로 받아온 데이터(이미지, target)를 batch로 묶어 return합니다.\n",
        "    from torch.utils.data.sampler import RandomSampler\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=4, sampler=RandomSampler(dataset_train), num_workers=0)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset_val, batch_size=4, num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=4, num_workers=0)\n",
        "    print('데이터 로더 완료!')\n",
        "    #### 데이터 준비 파트는 마무리가 되었습니다. 큰 틀을 살펴보면서 정리해보도록 하겠습니다.\n",
        "    # 0. 데이터셋 다운받기`: 여러분이 수집한 데이터의 클래스별로 폴더를 구성하여 데이터셋을 준비합니다.\n",
        "    # 1. 데이터셋 구성하기`: 저장한 데이터의 정보를 csv 파일로 만듭니다.\n",
        "    # 2. 데이터셋 불러오기`: csv 파일을 통해 데이터를 불러와서 train, validation, test로 나눠줍니다.\n",
        "    # 3. 학습 시, 데이터셋을 사용할 수 있도록 만들기\n",
        "    #     3-1. Dataset Class`: pytorch가 dataset을 어떻게 읽을지 알려주는 클래스를 만듭니다. (데이터셋 크기와 지정한 인덱스별로 데이터를 리턴해주는 len, getitem 함수가 포함되어 있습니다.)\n",
        "    #     3-2. Transforms & Augmentation`: 학습을 위해 데이터를 가공합니다.\n",
        "    #     3-3. Data Loaders`: 배치별로 데이터를 묶어줍니다. Training시, 배치단위별로 데이터가 호출됩니다.\n",
        "\n",
        "    # Model 설정\n",
        "    from torchvision import models\n",
        "    from collections import OrderedDict\n",
        "    import torch.nn as nn\n",
        "\n",
        "    model = models.vgg19(pretrained=True)\n",
        "    # # Backprop을 수행하지 않도록 parameter들을 동결시키기\n",
        "    # # 재학습을 위해, 모든 parameters의 gradient를 꺼놓기\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 마지막 layer를 과제에 맞게 수정하기\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(25088, 500)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('fc2', nn.Linear(500, 10))\n",
        "    ]))\n",
        "\n",
        "    model.classifier = classifier\n",
        "    print('VGG19 모델 셋팅 완료')\n",
        "\n",
        "    # Training\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    import random\n",
        "    import time\n",
        "    import torch.optim as optim\n",
        "\n",
        "    from tqdm import tqdm # tqdm은 작업의 진행률을 시각적으로 표시해준다.\n",
        "\n",
        "    # train\n",
        "    def train_epoch(model, loader, device, criterion, optimizer):\n",
        "        model.train() # 모델 train 모드로 바꾸기\n",
        "        train_loss = []\n",
        "        bar = tqdm(loader)\n",
        "\n",
        "        for i, (data, target) in enumerate(bar):\n",
        "            optimizer.zero_grad() # 최적화된 모든 변수 초기화\n",
        "\n",
        "            data, target = data.to(device), target.to(device) # 지정한 device로 데이터 옮기기\n",
        "            logits = model(data) # 1. forward pass\n",
        "\n",
        "            loss = criterion(logits, target) # 2. loss계산\n",
        "            loss.backward() # 3. backward pass\n",
        "            optimizer.step() # 4. gradient descent(파라미터 업데이트)\n",
        "\n",
        "            loss_np = loss.detach().cpu().numpy() # loss값 가져오기 위해 gpu에 있던 데이터 모두 cpu로 옮기기\n",
        "            train_loss.append(loss_np)\n",
        "            bar.set_description('loss: %.5f' % (loss_np))\n",
        "\n",
        "        train_loss = np.mean(train_loss) # 한 epoch당 train loss의 평균 구하기\n",
        "        return train_loss\n",
        "\n",
        "    # Validation\n",
        "    def val_epoch(model, loader, device, criterion):\n",
        "        model.eval() # 모델 evaluate 모드로 바꾸기\n",
        "        val_loss = []\n",
        "        LOGITS = []\n",
        "        PROBS = []\n",
        "        TARGETS = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (data, target) in tqdm(loader):\n",
        "\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                logits = model(data)    # 1. forward pass\n",
        "                probs = logits.softmax(1)\n",
        "\n",
        "                LOGITS.append(logits.detach().cpu())\n",
        "                PROBS.append(probs.detach().cpu())\n",
        "                TARGETS.append(target.detach().cpu())\n",
        "\n",
        "                loss = criterion(logits, target)    # 2. loss 계산\n",
        "                val_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        val_loss = np.mean(val_loss)\n",
        "        LOGITS = torch.cat(LOGITS).numpy()\n",
        "        PROBS = torch.cat(PROBS).numpy()\n",
        "        TARGETS = torch.cat(TARGETS).numpy()\n",
        "\n",
        "        # accuracy: 정확도\n",
        "        acc = (PROBS.argmax(1) == TARGETS).mean() * 100.\n",
        "\n",
        "        return val_loss, acc\n",
        "    \n",
        "    # 학습시키기\n",
        "    def run(model = model, init_lr = 4e-6, n_epochs = 15):\n",
        "        # gpu 사용\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"현재 장치: {device}\")\n",
        "        # model을 지정한 장치로 옮기기\n",
        "        model = model.to(device)\n",
        "\n",
        "        # loss function 지정\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # optimizer로 adam 사용\n",
        "        optimizer = optim.Adam(model.parameters(), lr = init_lr)\n",
        "\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(time.ctime(), f'Epoch {epoch}')\n",
        "\n",
        "            train_loss = train_epoch(model, train_loader, device, criterion, optimizer) # train\n",
        "            val_loss, acc = val_epoch(model, valid_loader, device, criterion) # validation\n",
        "\n",
        "            content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}, Acc: {(acc):.4f}.'\n",
        "            print(content)\n",
        "\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    run(model, init_lr=4e-6, n_epochs=epochNum)\n",
        "    print(\"가장 좋은 성능의 모델 저장 완료!\")\n",
        "    print(\"학습 종료\");\n",
        "\n",
        "    print(\"\\n테스트셋으로 최종 정확도를 계산합니다.\")\n",
        "\n",
        "    def test_epoch(model, loader, device, criterion):\n",
        "        model.eval() # 모델 evaluate 모드로 바꾸기\n",
        "        val_loss = []\n",
        "        LOGITS = []\n",
        "        PROBS = []\n",
        "        TARGETS = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (data, target) in tqdm(loader):\n",
        "\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                logits = model(data)    # 1. forward pass\n",
        "                probs = logits.softmax(1)\n",
        "\n",
        "                LOGITS.append(logits.detach().cpu())\n",
        "                PROBS.append(probs.detach().cpu())\n",
        "                TARGETS.append(target.detach().cpu())\n",
        "\n",
        "                loss = criterion(logits, target)    # 2. loss 계산\n",
        "                val_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        val_loss = np.mean(val_loss)\n",
        "        LOGITS = torch.cat(LOGITS).numpy()\n",
        "        PROBS = torch.cat(PROBS).numpy()\n",
        "        TARGETS = torch.cat(TARGETS).numpy()\n",
        "\n",
        "        # accuracy: 정확도\n",
        "        acc = (PROBS.argmax(1) == TARGETS).mean() * 100.\n",
        "\n",
        "        return val_loss, acc\n",
        "    \n",
        "    def run_test(model = model, init_lr = 4e-6, n_epochs = 1):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # loss function 지정\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # optimizer로 adam 사용\n",
        "        optimizer = optim.Adam(model.parameters(), lr = init_lr)\n",
        "\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(time.ctime(), f'Epoch {epoch}')\n",
        "\n",
        "            train_loss = train_epoch(model, train_loader, device, criterion, optimizer) # train\n",
        "            test_loss, acc = test_epoch(model, test_loader, device, criterion) # test\n",
        "\n",
        "            content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, test loss: {(val_loss):.5f}, Acc: {(acc):.4f}.'\n",
        "            print(content)\n",
        "\n",
        "    run_test(model)\n",
        "elif menu == '1':\n",
        "    pass\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FingerNumber_Classifier\n",
            "[0] 모델 새로 학습하기\n",
            "[1] 학습된 모델로 새로운 사진 분류해보기\n",
            "[any] 종료\n",
            "숫자를 입력해주세요: 0\n",
            "모델을 새로 학습합니다.\n",
            "15 에포크 기준 정확도는 약 79%입니다.\n",
            "원하는 에포크 수를 입력해주세요: 30\n",
            "30으로 에포크를 설정하였습니다.\n",
            "모델 학습을 시작합니다.\n",
            "csv파일 생성 완료!\n",
            "csv 파일 DataFrame으로 저장 완료!\n",
            "훈련셋, 검증셋, 테스트셋 분할 완료!\n",
            "훈련셋 개수:192, 검증셋 개수:48, 테스트셋 개수: 60\n",
            "rgb의 mean, std값 계산 완료!\n",
            "모델 트랜스폼 불러오기 완료!\n",
            "dataset class 객체 생성 완료!\n",
            "데이터 로더 완료!\n",
            "VGG19 모델 셋팅 완료\n",
            "현재 장치: cuda\n",
            "Mon Nov 22 10:59:28 2021 Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.27220: 100%|██████████| 48/48 [00:07<00:00,  6.06it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 10:59:38 2021 Epoch 1, lr: 0.0000040, train loss: 2.38563, valid loss: 2.29735, Acc: 14.5833.\n",
            "Mon Nov 22 10:59:38 2021 Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.33933: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 10:59:47 2021 Epoch 2, lr: 0.0000040, train loss: 2.22676, valid loss: 2.07623, Acc: 27.0833.\n",
            "Mon Nov 22 10:59:47 2021 Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.57054: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 10:59:57 2021 Epoch 3, lr: 0.0000040, train loss: 2.05291, valid loss: 1.91473, Acc: 39.5833.\n",
            "Mon Nov 22 10:59:57 2021 Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.98703: 100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:07 2021 Epoch 4, lr: 0.0000040, train loss: 1.94952, valid loss: 1.77020, Acc: 41.6667.\n",
            "Mon Nov 22 11:00:07 2021 Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.02185: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:16 2021 Epoch 5, lr: 0.0000040, train loss: 1.85568, valid loss: 1.63960, Acc: 54.1667.\n",
            "Mon Nov 22 11:00:16 2021 Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.91520: 100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:26 2021 Epoch 6, lr: 0.0000040, train loss: 1.81364, valid loss: 1.53995, Acc: 62.5000.\n",
            "Mon Nov 22 11:00:26 2021 Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.90859: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:36 2021 Epoch 7, lr: 0.0000040, train loss: 1.72179, valid loss: 1.44284, Acc: 62.5000.\n",
            "Mon Nov 22 11:00:36 2021 Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.25413: 100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:45 2021 Epoch 8, lr: 0.0000040, train loss: 1.64556, valid loss: 1.37407, Acc: 62.5000.\n",
            "Mon Nov 22 11:00:45 2021 Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.04444: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:00:55 2021 Epoch 9, lr: 0.0000040, train loss: 1.50144, valid loss: 1.28854, Acc: 60.4167.\n",
            "Mon Nov 22 11:00:55 2021 Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.50888: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:05 2021 Epoch 10, lr: 0.0000040, train loss: 1.44083, valid loss: 1.20792, Acc: 66.6667.\n",
            "Mon Nov 22 11:01:05 2021 Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.24966: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:14 2021 Epoch 11, lr: 0.0000040, train loss: 1.37524, valid loss: 1.11592, Acc: 68.7500.\n",
            "Mon Nov 22 11:01:14 2021 Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.63675: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:24 2021 Epoch 12, lr: 0.0000040, train loss: 1.27467, valid loss: 1.05027, Acc: 72.9167.\n",
            "Mon Nov 22 11:01:24 2021 Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.17730: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:33 2021 Epoch 13, lr: 0.0000040, train loss: 1.29239, valid loss: 0.99794, Acc: 72.9167.\n",
            "Mon Nov 22 11:01:33 2021 Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.57258: 100%|██████████| 48/48 [00:07<00:00,  6.11it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:43 2021 Epoch 14, lr: 0.0000040, train loss: 1.25974, valid loss: 0.94810, Acc: 68.7500.\n",
            "Mon Nov 22 11:01:43 2021 Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.02199: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:01:53 2021 Epoch 15, lr: 0.0000040, train loss: 1.09896, valid loss: 0.88951, Acc: 77.0833.\n",
            "Mon Nov 22 11:01:53 2021 Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.41604: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:02 2021 Epoch 16, lr: 0.0000040, train loss: 1.13476, valid loss: 0.88730, Acc: 70.8333.\n",
            "Mon Nov 22 11:02:02 2021 Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.91309: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:12 2021 Epoch 17, lr: 0.0000040, train loss: 1.06401, valid loss: 0.83945, Acc: 75.0000.\n",
            "Mon Nov 22 11:02:12 2021 Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.85226: 100%|██████████| 48/48 [00:07<00:00,  6.13it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:22 2021 Epoch 18, lr: 0.0000040, train loss: 1.00157, valid loss: 0.82006, Acc: 72.9167.\n",
            "Mon Nov 22 11:02:22 2021 Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.64092: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:32 2021 Epoch 19, lr: 0.0000040, train loss: 1.00145, valid loss: 0.77766, Acc: 77.0833.\n",
            "Mon Nov 22 11:02:32 2021 Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.94184: 100%|██████████| 48/48 [00:07<00:00,  6.11it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:41 2021 Epoch 20, lr: 0.0000040, train loss: 0.97642, valid loss: 0.76798, Acc: 75.0000.\n",
            "Mon Nov 22 11:02:41 2021 Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.32500: 100%|██████████| 48/48 [00:07<00:00,  6.09it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:02:51 2021 Epoch 21, lr: 0.0000040, train loss: 0.85668, valid loss: 0.74741, Acc: 77.0833.\n",
            "Mon Nov 22 11:02:51 2021 Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.55553: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:01 2021 Epoch 22, lr: 0.0000040, train loss: 0.78040, valid loss: 0.72532, Acc: 79.1667.\n",
            "Mon Nov 22 11:03:01 2021 Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.24596: 100%|██████████| 48/48 [00:07<00:00,  6.17it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:10 2021 Epoch 23, lr: 0.0000040, train loss: 0.87970, valid loss: 0.76131, Acc: 72.9167.\n",
            "Mon Nov 22 11:03:10 2021 Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.83058: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:20 2021 Epoch 24, lr: 0.0000040, train loss: 0.74953, valid loss: 0.70072, Acc: 79.1667.\n",
            "Mon Nov 22 11:03:20 2021 Epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.11847: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:30 2021 Epoch 25, lr: 0.0000040, train loss: 0.81546, valid loss: 0.70987, Acc: 77.0833.\n",
            "Mon Nov 22 11:03:30 2021 Epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.50305: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:39 2021 Epoch 26, lr: 0.0000040, train loss: 0.83967, valid loss: 0.74014, Acc: 75.0000.\n",
            "Mon Nov 22 11:03:39 2021 Epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.69829: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:49 2021 Epoch 27, lr: 0.0000040, train loss: 0.77432, valid loss: 0.70634, Acc: 79.1667.\n",
            "Mon Nov 22 11:03:49 2021 Epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.80729: 100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:03:59 2021 Epoch 28, lr: 0.0000040, train loss: 0.72115, valid loss: 0.66576, Acc: 81.2500.\n",
            "Mon Nov 22 11:03:59 2021 Epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.41791: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:04:08 2021 Epoch 29, lr: 0.0000040, train loss: 0.89566, valid loss: 0.68236, Acc: 83.3333.\n",
            "Mon Nov 22 11:04:08 2021 Epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.85123: 100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 22 11:04:18 2021 Epoch 30, lr: 0.0000040, train loss: 0.77982, valid loss: 0.68189, Acc: 83.3333.\n",
            "가장 좋은 성능의 모델 저장 완료!\n",
            "학습 종료\n",
            "\n",
            "테스트셋으로 최종 정확도를 계산합니다.\n",
            "Mon Nov 22 11:04:18 2021 Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.46740: 100%|██████████| 48/48 [00:07<00:00,  6.06it/s]\n",
            "  0%|          | 0/15 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bdc2624a5371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mmenu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bdc2624a5371>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(model, init_lr, n_epochs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, test loss: {(val_loss):.5f}, Acc: {(acc):.4f}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bdc2624a5371>\u001b[0m in \u001b[0;36mtest_epoch\u001b[0;34m(model, loader, device, criterion)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
          ]
        }
      ]
    }
  ]
}